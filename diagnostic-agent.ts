import { inspector } from "./core-inspector.js";
import { InspectorBlockData } from "./index.d.js";

/**
 * Interface para a resposta estruturada da IA.
 */
export interface DecisionResponse {
  action:
    | "CLEAN_CACHE"
    | "SCALE_WORKERS"
    | "REJECT_TRAFFIC"
    | "NONE"
    | "SCALE_DOWN_WORKERS"
    | "SCALE_UP_WORKERS";
  reason: string;
  intensity?: number; // 1-10, para aﾃｧﾃｵes mais graduais
}

/**
 * Consulta o Ollama local com um prompt de SRE.
 */
let isAnalyzing = false;
let lastDecisionTime = 0;
const COOLDOWN_MS = 10000; // 10 segundos de espera entre decisﾃｵes da IA

export async function askOllamaDecision(
  data: InspectorBlockData,
): Promise<DecisionResponse | null> {
  const now = Date.now();

  if (isAnalyzing || now - lastDecisionTime < COOLDOWN_MS) {
    //evita sobrecarga no modelo
    return null;
  }

  isAnalyzing = true;

  const prompt = `
    Vocﾃｪ ﾃｩ um Orquestrador de Recursos Autﾃｴnomo.
    ESTADO ATUAL:
    - Funﾃｧﾃ｣o: ${data.function}
    - Lag: ${data.blockDuration}ms
    - Memﾃｳria: ${data.memoryUsage}
    - I/O Ativo: ${data.activeRequests}

    OBJETIVO: Encontrar o equilﾃｭbrio entre performance e custo.
    REGRAS:
    - Se Lag > 50ms constante: 'SCALE_UP_WORKERS'
    - Se Lag < 15ms por muito tempo e Memﾃｳria estﾃ｡vel: 'SCALE_DOWN_WORKERS' (Economia)
    - Se Memﾃｳria > 85%: 'CLEAN_CACHE'
    - Se tudo estﾃ｡ normal: 'NONE'

    Responda apenas JSON: {"action": "string", "reason": "string", "intensity": number}
  `;

  try {
    const res = await fetch("http://localhost:11434/api/generate", {
      method: "POST",
      body: JSON.stringify({
        model: "llama3",
        prompt: prompt,
        format: "json",
        stream: false,
      }),
      signal: AbortSignal.timeout(30000),
    });

    const result = await res.json();
    lastDecisionTime = Date.now();
    return JSON.parse(result.response);
  } catch (e) {
    console.error("Ollama indisponﾃｭvel ou Timeout");
    return null;
  } finally {
    isAnalyzing = false;
  }
}

/**
 * Executa a aﾃｧﾃ｣o sugerida pela IA no ambiente Node.js.
 */
export function executeAction(decision: DecisionResponse) {
  console.log(
    `[Agente] IA Decidiu: ${decision.action} (Intensidade: ${decision.intensity}) - ${decision.reason}`,
  );

  switch (decision.action) {
    case "SCALE_WORKERS":
      console.log("[Aﾃﾃグ] Escalando Workers (Implementaﾃｧﾃ｣o pendente)...");
      break;
    case "SCALE_UP_WORKERS":
      console.log("噫 Aumentando threads para processamento paralelo");
      // Sua lﾃｳgica de pool.addWorker()
      break;

    case "SCALE_DOWN_WORKERS":
      console.log("悼 Reduzindo threads ociosas para economizar recursos");
      // Sua lﾃｳgica de pool.removeWorker() ou pool.terminate()
      break;
    case "CLEAN_CACHE":
      if (global.gc) {
        console.log("[Aﾃﾃグ] Forﾃｧando GC para liberar memﾃｳria...");
        global.gc();
      } else {
        console.warn("[Aﾃﾃグ] GC nﾃ｣o disponﾃｭvel. Rode Node com --expose-gc.");
      }
      break;
    case "REJECT_TRAFFIC":
      console.log("[Aﾃﾃグ] Ativando Circuit Breaker para rejeitar trﾃ｡fego...");
      break;
    case "NONE":
    default:
      console.log("[Aﾃﾃグ] Nenhuma aﾃｧﾃ｣o necessﾃ｡ria no momento.");
      break;
  }
}

// Inicia o Inspetor quando este mﾃｳdulo ﾃｩ carregado
inspector.start({
  block: 5000, // Tempo de bloqueio em ms para considerar alerta
  heap: 85, // Porcentagem de uso de heap para alerta
  io: 50, // Nﾃｺmero de requisiﾃｧﾃｵes de I/O ativas para alerta
  criticalFunctions: ["expensiveCalculation"], // Nomes de funﾃｧﾃｵes a monitorar especificamente
});
console.log("噫 Agente de Diagnﾃｳstico e Orquestraﾃｧﾃ｣o AI Ativo.");
